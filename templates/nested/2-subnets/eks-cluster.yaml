AWSTemplateFormatVersion: "2010-09-09"
Description: EKSCluster stack that create EKS cluster

Metadata:
    AWS::CloudFormation::Interface:
      ParameterGroups:
        - Label:
            default: Cluster Configuration
          Parameters:
            - RemoveTempResources
            - EksMgmtIamRoleArn
            - EksMgmtIamUserArnList
            - NodeInstanceRoleArn
            - EKSServiceRoleArn
            - EKSClusterName
            - KubernetesVersion
            - Environment
            - TemplateBucketName
            - ControlPanelSecurityGroup
            - NodeSecurityGroup
            - ProvisionSecurityGroup
            - ElbInboundCIDRs
        - Label:
            default: Provision Stack Configuration
          Parameters:
            - VPCID
            - PrivateSubnet1ID
            - PrivateSubnet2ID
            - PublicSubnet1ID
            - PublicSubnet2ID
            - EC2LogGroup
            - ProvisionInstanceType

      ParameterLabels:
        RemoveTempResources:
          default: Enable Deletion of Temp Resources
        TemplateBucketName:
          default: The name of the S3 bucket that holds the templates
        VPCID:
          default: The ID of the VPC to deploy the Provision and EKS Cluster into
        PrivateSubnet1ID:
          default: The ID of the first private subnet to deploy EKS Workers into
        PrivateSubnet2ID:
          default: The ID of the second private subnet to deploy EKS Workers into
        PublicSubnet1ID:
          default: The ID of the first public subet to deploy EKS into
        PublicSubnet2ID:
          default: The ID of the second public subnet to deploy EKS into
        EC2LogGroup:
          default: The provision log group name
        ProvisionInstanceType:
          default: The instance type to deploy Provision to
        NodeInstanceRoleArn:
          default: The AWS IAM Role ARN to be applied to the EKS Worker Nodes
        EKSServiceRoleArn:
          default: The AWS IAM Role ARN to be applied to the EKS Control Panel
        NodeSecurityGroup:
          default: The Security Group of EKS Worker nodes
        ProvisionSecurityGroup:
          default: The Security Group of Provision Host
        ControlPanelSecurityGroup:
          default: The Security Group of EKS Control Panel
        EksMgmtIamRoleArn:
          default: The AWS IAM Role name that will be allowed to manage EKS. Note format is arn:aws:iam::123456789:role/admin-role
        EksMgmtIamUserArnList:
          default: The AWS IAM user arns list, who will be authorised to connect the cluster
        EKSClusterName:
          default: The EKS cluster name
        KubernetesVersion:
          default: The Kubernetes Version
        Environment:
          default: The Environment
        ElbInboundCIDRs:
          default: DataHub ALB Inbound CIDRs

Parameters:
    RemoveTempResources:
      Description: "Set to true if you don't want to keep provision host"
      Type: String
      Default: "true"
      AllowedValues:
        - "true"
        - "false"
    TemplateBucketName:
      AllowedPattern: "^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$"
      ConstraintDescription: "Bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-)."
      Description: "S3 bucket name that contains the CFN templates (VPC, Provision etc). This string can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-)."
      Type: "String"
    VPCID:
      Description: "ID for the VPC"
      Type: "AWS::EC2::VPC::Id"
    PublicSubnet1ID:
      Description: "ID of Public Subnet 1"
      Type: "AWS::EC2::Subnet::Id"
    PublicSubnet2ID:
      Description: "ID of Public Subnet 2"
      Type: "AWS::EC2::Subnet::Id"
    PrivateSubnet1ID:
      Description: "ID of Private Subnet 1"
      Type: "AWS::EC2::Subnet::Id"
    PrivateSubnet2ID:
      Description: "ID of Private Subnet 2"
      Type: "AWS::EC2::Subnet::Id"
    EC2LogGroup:
      Description: The provision log group name
      Type: "String"
    ProvisionInstanceType:
      Type: "String"
      Description: "The type of EC2 instance to be launched for Provision Host"
      AllowedValues:
        # Add more instance types if needed
        - t2.micro
        - t2.medium
        - t2.large
      ConstraintDescription: "Must contain a valid instance type"
    NodeInstanceRoleArn:
      Type: "String"
      Description: "The AWS IAM Role ARN to be applied to the EKS Worker Nodes"
    EKSServiceRoleArn:
      Type: "String"
      Description: "The AWS IAM Role ARN to be applied to the EKS Control Panel"
    ControlPanelSecurityGroup:
      Description: "ID for the VPC, This will be used to get the eks control panel security group"
      Type: "AWS::EC2::SecurityGroup::Id"
    NodeSecurityGroup:
      Description: "ID for the VPC, This will be used to get the node security group"
      Type: "AWS::EC2::SecurityGroup::Id"
    ProvisionSecurityGroup:
      Description: "ID for the VPC, This will be used to get the provision security group"
      Type: "AWS::EC2::SecurityGroup::Id"
    EksMgmtIamRoleArn:
      Type: String
      Description: "The AWS IAM Role name that will be allowed to manage EKS. Note format is arn:aws:iam::123456789:role/admin-role"
    EksMgmtIamUserArnList:
      Type: String
      Description: "The AWS IAM user arns list who will be authorised to connect the cluster. Note format is arn:aws:iam::123456789:user/user1,arn:aws:iam::123456789:user/user2"
    EKSClusterName:
      Type: String
      Description: The name of the eks cluster
    KubernetesVersion:
      Type: String
      Description: The Kubernetes Version
    Environment:
      Type: String
      Description: The Environment
    ElbInboundCIDRs:
      ConstraintDescription: The Elastic Load Balancer Inbound CIDRs
      Type: String
      Default: "10.0.0.0/8,192.0.0.0/8,172.0.0.0/8"

Mappings:
  # Use Amazon 2 Linux
  ProvisionLatestAmiRegionMap:
    us-west-2:
      AmiId: ami-0873b46c45c11058d
    us-west-1:
      AmiId: ami-05655c267c89566dd
    us-east-1:
      AmiId: ami-02354e95b39ca8dec
    us-east-2:
      AmiId: ami-07c8bc5c1ce9598c3
    eu-central-1:
      AmiId: ami-0c115dbd34c69a004
    eu-west-1:
      AmiId: ami-07d9160fa81ccffb5

Resources:
  ProvisionInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${EKSClusterName}-provision-role"
      Policies:
        - PolicyName: !Sub "${TemplateBucketName}-${Environment}-s3-policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - s3:GetObject
                Resource: !Sub "arn:aws:s3:::${TemplateBucketName}/${Environment}/scripts/*"
                Effect: Allow
        - PolicyName: cloudwatch-logs-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Action:
                  - logs:CreateLogStream
                  - logs:GetLogEvents
                  - logs:PutLogEvents
                Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${EC2LogGroup}:*"
                Effect: Allow
        - PolicyName: provision-ssm-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - ssm:PutParameter
                  - ssm:GetParameters
                  - ssm:GetParameter
                  - ssm:AddTagsToResource
                Resource: !Sub "arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${Environment}/${EKSClusterName}/*"
                #Resource: "*"
                Effect: Allow
        - PolicyName: provision-OpenID-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - iam:GetOpenIDConnectProvider
                  - iam:CreateOpenIDConnectProvider
                  - iam:TagOpenIDConnectProvider
                Resource: "*"
                Effect: Allow
        - PolicyName: provision-secretsmanager-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - secretsmanager:CreateSecret
                  - secretsmanager:GetSecretValue
                  - secretsmanager:TagResource 
                Resource: !Sub "arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:/${Environment}/${EKSClusterName}/*"
                #Resource: "*"
                Effect: Allow
        - PolicyName: provision-secretsmanager-desc-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - secretsmanager:DescribeSecret
                Resource: "*"
                Effect: Allow
        - PolicyName: provision-iam-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - iam:PassRole
                  - iam:GetRole
                Resource: "*"
                Effect: Allow
        - PolicyName: provision-kafka-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - kafka:Describe*
                  - kafka:Get*
                  - kafka:List*
                Resource: "*"
                Effect: Allow
        - PolicyName: provision-autoscaling-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - autoscaling:DescribeAutoScalingInstances
                  - autoscaling:DescribeAutoScalingGroups
                  - autoscaling:DescribeScalingActivities
                Resource: "*"
                Effect: Allow
        - PolicyName: provision-autoscaling-update-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - autoscaling:UpdateAutoScalingGroup
                Resource: !Sub "arn:aws:autoscaling:${AWS::Region}:${AWS::AccountId}:autoScalingGroup:*:autoScalingGroupName/${AWS::StackName}*"
                Effect: Allow
        - PolicyName: eks-cluster-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - eks:*
                Resource: !Sub "arn:aws:eks:${AWS::Region}:${AWS::AccountId}:cluster/${EKSClusterName}"
                #Resource: "*"
                Effect: Allow
        - PolicyName: provision-others-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - ec2:DescribeTags 
                  - ec2:DescribeSecurityGroups
                  - ec2:AuthorizeSecurityGroupIngress
                  - elasticloadbalancing:DescribeLoadBalancers
                  - elasticloadbalancing:DescribeTags
                  - elasticloadbalancing:DescribeTargetGroups
                  #- sts:*
                Resource: "*"
                Effect: Allow
      Path: /
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Principal:
              Service:
                - ec2.amazonaws.com
            Effect: Allow
        Version: "2012-10-17"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM

  ProvisionInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref ProvisionInstanceRole
      Path: /

  EKSProvisionAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref EKSProvisionLaunchConfiguration
      VPCZoneIdentifier:
        - !Ref PrivateSubnet1ID
        - !Ref PrivateSubnet2ID
      MinSize: 1
      MaxSize: 1
      Cooldown: "300"
      DesiredCapacity: 1
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-eks-provision-node"
          PropagateAtLaunch: true
        - Key: Component
          Value: !Sub "${EKSClusterName}-eks-provision-asg"
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT50M

  EKSProvisionLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Authentication:
        S3AccessCreds:
          type: S3
          roleName: !Ref ProvisionInstanceRole
          buckets: !Ref TemplateBucketName
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              awslogs: []
          files:
            '/etc/awslogs/awscli.conf':
              content: !Sub |
                [default]
                region = ${AWS::Region}
                [plugins]
                cwlogs = cwlogs
              mode: '000644'
              owner: root
              group: root
            '/etc/awslogs/awslogs.conf':
              content: !Sub |
                [general]
                state_file = /var/lib/awslogs/agent-state
                [/var/log/messages]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/messages
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/var/log/messages
                log_group_name = ${EC2LogGroup}
                [/var/log/secure]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/secure
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/var/log/secure
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-init.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-init.log
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/var/log/cfn-init.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-init-cmd.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-init-cmd.log
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/var/log/cfn-init-cmd.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cloud-init-output.log]
                file = /var/log/cloud-init-output.log
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/var/log/cloud-init-output.log
                log_group_name = ${EC2LogGroup}
                [/tmp/eks_bootstrap.output]
                file = /tmp/eks_bootstrap.output
                log_stream_name = ${AWS::StackName}/provision-{instance_id}/tmp/eks_bootstrap.output
                log_group_name = ${EC2LogGroup}
              mode: '000644'
              owner: root
              group: root
            /tmp/check_extra_tags.sh:
              source: !Sub "https://s3.${AWS::Region}.amazonaws.com/${TemplateBucketName}/${Environment}/scripts/check_extra_tags.sh"
              mode: "000750"
              owner: root
              group: root
            /tmp/check_extra_tags-v2.sh:
              source: !Sub "https://s3.${AWS::Region}.amazonaws.com/${TemplateBucketName}/${Environment}/scripts/check_extra_tags-v2.sh"
              mode: "000750"
              owner: root
              group: root
            /tmp/eks_bootstrap.sh:
              content: !Sub |
                #!/bin/bash
                set -x
                echo "Checking extra tags..."
                EXTRA_TAGS=$(/tmp/check_extra_tags.sh ${Environment} ${EKSClusterName})
                len=`expr length "$EXTRA_TAGS"`
                if [ $len -gt 3 ]; then
                  EXTRA_TAGS_FLAG="true"
                fi

                echo "Checking whether cluster exists..."
                aws eks describe-cluster --region ${AWS::Region} --name ${EKSClusterName} &> /dev/null
                if [ $? -ne 0 ]; then
                  if [ "$EXTRA_TAGS_FLAG" == "true" ]; then
                  echo "Cluster does not exist, creating with EXTRA_TAGS..."
                  aws eks create-cluster --region ${AWS::Region} \
                  --name ${EKSClusterName} \
                  --tags "$EXTRA_TAGS" \
                  --kubernetes-version ${KubernetesVersion} \
                  --role-arn ${EKSServiceRoleArn} \
                  --resources-vpc-config subnetIds=${PrivateSubnet1ID},${PrivateSubnet2ID},${PublicSubnet1ID},${PublicSubnet2ID},securityGroupIds=${ControlPanelSecurityGroup},endpointPublicAccess=false,endpointPrivateAccess=true \
                  --logging '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":true}]}'
                  else
                  echo "Cluster does not exist, creating..."
                  aws eks create-cluster --region ${AWS::Region} \
                  --name ${EKSClusterName} \
                  --kubernetes-version ${KubernetesVersion} \
                  --role-arn ${EKSServiceRoleArn} \
                  --resources-vpc-config subnetIds=${PrivateSubnet1ID},${PrivateSubnet2ID},${PublicSubnet1ID},${PublicSubnet2ID},securityGroupIds=${ControlPanelSecurityGroup},endpointPublicAccess=false,endpointPrivateAccess=true \
                  --logging '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":true}]}'
                  fi 
                  if [ $? -ne 0 ]; then
                    exit 1
                  fi
                  sleep 5
                  STATUS=$(aws eks describe-cluster --region ${AWS::Region} --name ${EKSClusterName} --query cluster.status --output text)
                  while [ \"$STATUS\" = \"CREATING\" ]; do
                    echo "Cluster is still creating, sleeping for 30 seconds..."
                    sleep 30
                    STATUS=$(aws eks describe-cluster --region ${AWS::Region} --name ${EKSClusterName} --query cluster.status --output text)
                  done
                fi
                echo "Saving oidc info..."
                OIDC=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.identity.oidc --output text --no-paginate |cut -c 9-)
                parameter="/${Environment}/${EKSClusterName}/eks/oidc"
                if [ "$EXTRA_TAGS_FLAG" == "true" ]; then
                  EXTRA_TAGS_V2=$(/tmp/check_extra_tags-v2.sh ${Environment} ${EKSClusterName})
                  aws ssm put-parameter --tags $EXTRA_TAGS_V2 --region ${AWS::Region} --name "$parameter" --type "String" --value "$OIDC" --no-paginate
                else
                  aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "$OIDC" --overwrite --no-paginate
                fi
                EKSSecurityGroupId=$(aws eks describe-cluster --region ${AWS::Region} --name ${EKSClusterName} --query cluster.resourcesVpcConfig.securityGroupIds --output text)
                echo "add ingress rule to EKS control panel security group..."
                for cidr in $(echo ${ElbInboundCIDRs} | sed "s/,/ /g")
                do
                  aws ec2 authorize-security-group-ingress --region ${AWS::Region} --group-id $EKSSecurityGroupId --ip-permissions IpProtocol=tcp,FromPort=443,ToPort=443,IpRanges="[{CidrIp=$cidr,Description='grant access to EKS'}]"
                done
                echo "Saving security group info..."
                clusterSecurityGroupId=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.resourcesVpcConfig.clusterSecurityGroupId  --output text)
                parameter="/${Environment}/${EKSClusterName}/eks/clusterSecurityGroupId"
                if [ "$EXTRA_TAGS_FLAG" == "true" ]; then
                  EXTRA_TAGS_V2=$(/tmp/check_extra_tags-v2.sh ${Environment} ${EKSClusterName})
                  aws ssm put-parameter --tags $EXTRA_TAGS_V2 --region ${AWS::Region} --name "$parameter" --type "String" --value "$clusterSecurityGroupId" --no-paginate
                else
                  aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "$clusterSecurityGroupId" --overwrite --no-paginate
                fi
                set +x
                echo "Updating kubeconfig file..."
                ENDPOINT=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.endpoint --output text)
                CERT_DATA=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.certificateAuthority.data --output text)
                sed -i s,ENDPOINT,$ENDPOINT,g /home/ec2-user/.kube/config
                sed -i s,CERTIFICATE_DATA,$CERT_DATA,g /home/ec2-user/.kube/config
                export KUBECONFIG=/home/ec2-user/.kube/config
                echo "Checking whether aws-auth configmap exists..."
                kubectl get configmaps/aws-auth -n kube-system &> /dev/null
                if [ $? -gt 0 ]; then
                  echo "Configmap does not exist, applying..."
                  curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                  for u in $(echo ${EksMgmtIamUserArnList} | sed "s/,/ /g")
                  do
                    echo "    - userarn: $u" >> /tmp/aws-auth-cm.yaml
                    echo "      username: admin" >> /tmp/aws-auth-cm.yaml
                    echo "      groups:" >> /tmp/aws-auth-cm.yaml
                    echo "        - system:masters" >> /tmp/aws-auth-cm.yaml
                  done
                  kubectl apply -f /tmp/aws-auth-cm.yaml
                  mv /tmp/eksctl /usr/local/bin
                  eksctl create iamidentitymapping  --region ${AWS::Region} --cluster ${EKSClusterName} --arn ${EksMgmtIamRoleArn} --username admin --group system:masters
                  kubectl get cm -n kube-system aws-auth -o yaml
                fi
              mode: "000750"
              owner: root
              group: root
            /tmp/aws-auth-cm.yaml:
              content: !Sub |
                apiVersion: v1
                kind: ConfigMap
                metadata:
                  name: aws-auth
                  namespace: kube-system
                data:
                  mapRoles: |
                    - rolearn: ${NodeInstanceRoleArn}
                      username: system:node:{{EC2PrivateDNSName}}
                      groups:
                        - system:bootstrappers
                        - system:nodes
                  mapUsers: |
              mode: "000644"
              owner: root
              group: root
            /home/ec2-user/.kube/config:
              content: !Sub |
                apiVersion: v1
                clusters:
                - cluster:
                    server: ENDPOINT
                    certificate-authority-data: CERTIFICATE_DATA
                  name: kubernetes
                contexts:
                - context:
                    cluster: kubernetes
                    user: aws
                  name: aws
                current-context: aws
                kind: Config
                preferences: {}
                users:
                - name: aws
                  user:
                    exec:
                      apiVersion: client.authentication.k8s.io/v1alpha1
                      command: aws-iam-authenticator
                      args:
                        - token
                        - -i
                        - ${EKSClusterName}
              mode: "000666"
              owner: ec2-user
              group: ec2-user
          commands:
            01_create_state_directory:
              command: mkdir -p /var/awslogs/state
            02_start_awslogsd:
              command: systemctl start awslogsd
            03_enable_awslogsd:
              command: systemctl enable awslogsd
            04_extra_tags:
              command: "./tmp/extra_tags.sh |tee -a /tmp/eks_bootstrap.output 2>&1"
            05_eks-bootstrap:
              command: "./tmp/eks_bootstrap.sh |tee -a /tmp/eks_bootstrap.output 2>&1"
    Properties:
      AssociatePublicIpAddress: false
      PlacementTenancy: default
      IamInstanceProfile: !Ref ProvisionInstanceProfile
      ImageId: !FindInMap [ProvisionLatestAmiRegionMap, !Ref "AWS::Region", AmiId]
      SecurityGroups:
        - !Ref ProvisionSecurityGroup
      InstanceType: !Ref ProvisionInstanceType
      UserData:
        Fn::Base64: !Sub |
            #!/bin/bash
            set -x
            export PATH=$PATH:/usr/local/bin:/opt/aws/bin
            yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
            yum install -y aws-cfn-bootstrap jq
            CLOUDWATCHGROUP=${EC2LogGroup}
            curl -LO https://dl.k8s.io/release/v1.20.2/bin/linux/amd64/kubectl
            chmod +x ./kubectl
            mv kubectl /usr/local/bin
            kubectl version --short --client
            curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/aws-iam-authenticator
            chmod +x ./aws-iam-authenticator
            mv aws-iam-authenticator /usr/local/bin
            aws-iam-authenticator version
            cfn-init -v --stack ${AWS::StackName} --resource EKSProvisionLaunchConfiguration --region ${AWS::Region}
            cfn-signal -e $? --stack ${AWS::StackName} --resource EKSProvisionAutoScalingGroup --region ${AWS::Region}
            echo "terminating eks provision node"
            INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            ASG=$(aws autoscaling describe-auto-scaling-instances --region ${AWS::Region} --output text --query=AutoScalingInstances[].AutoScalingGroupName --instance-ids=$INSTANCE_ID)
            echo "$ASG"
            nohup aws autoscaling --region ${AWS::Region} update-auto-scaling-group --auto-scaling-group-name $ASG --desired-capacity 0 --min-size 0 --max-size 0 &>/dev/null &


Outputs:
  SubstackName:
    Description: The eks stack name
    Value: !Sub "${AWS::StackName}"
  EksClusterName:
    Description: EKS Cluster name
    Value: !Ref EKSClusterName
  EKSProvisionLaunchConfiguration:
    Description: The provision host launch config
    Value: !Ref EKSProvisionLaunchConfiguration
  EKSProvisionAutoScalingGroup:
    Description: The Provision host autoscaling group
    Value: !Ref EKSProvisionAutoScalingGroup
  ProvisionInstanceProfile:
    Description: IAM Instance profile of Provision host
    Value: !Ref ProvisionInstanceProfile
  ProvisionInstanceRole:
    Description: IAM Role of Provision host
    Value: !Ref ProvisionInstanceRole
  NodeInstanceRoleArn:
    Value: !Ref NodeInstanceRoleArn
