AWSTemplateFormatVersion: "2010-09-09"
Description: "Bastion Stack that is used to deploy stuff to EKS."

Metadata:
    AWS::CloudFormation::Interface:
      ParameterGroups:
        - Label:
            default: Cluster Configuration
          Parameters:
            - EKSClusterName
            - K8sNamespace
            - TemplateBucketName
            - TemplateBucketKeyPrefix
        - Label:
            default: Bastion Stack Configuration
          Parameters:
            - VPCID
            - PrivateSubnet1ID
            - PrivateSubnet2ID
            - PublicSubnet1ID
            - PublicSubnet2ID
            - EC2LogGroup
            - KeyPairName
            - RemoteAccessCIDR
            - BastionInstanceType
            - MaxNumberOfBastionNodes
            - MinNumberOfBastionNodes
            - DesiredNumberOfBastionNodes

      ParameterLabels:
        TemplateBucketName:
          default: The name of the S3 bucket that holds the templates
        TemplateBucketKeyPrefix:
          default: The Key prefix for the templates in the S3 template bucket
        VPCID:
          default: The ID of the VPC to deploy the Bastion and EKS Cluster into
        PrivateSubnet1ID:
          default: The ID of the first private subnet to deploy EKS Workers into
        PrivateSubnet2ID:
          default: The ID of the second private subnet to deploy EKS Workers into
        PublicSubnet1ID:
          default: The ID of the first public subet to deploy EKS into
        PublicSubnet2ID:
          default: The ID of the second public subnet to deploy EKS into
        EC2LogGroup:
          default: The bastion log group name
        KeyPairName:
          default: The key pair name to use to access the instances
        RemoteAccessCIDR:
          default: The CIDR block to allow remote access
        BastionInstanceType:
          default: The instance type to deploy Bastion to
        MaxNumberOfBastionNodes:
          default: The maximum number of nodes to scale up to for Bastion
        MinNumberOfBastionNodes:
          default: The minimum number of nodes to scale down to for Bastion
        DesiredNumberOfBastionNodes:
          default: The desired number of nodes to keep running for Bastion
        EKSClusterName:
          default: The EKS cluster name
        K8sNamespace:
          default: The namespace in EKS to deploy kots and datahub app

Parameters:
    TemplateBucketName:
      AllowedPattern: "^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$"
      ConstraintDescription: "Bucket name can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-)."
      Description: "S3 bucket name that contains the CFN templates (VPC, Bastion etc). This string can include numbers, lowercase letters, uppercase letters, and hyphens (-). It cannot start or end with a hyphen (-)."
      Type: "String"
    TemplateBucketKeyPrefix:
      AllowedPattern: "^[0-9a-zA-Z-/]*$"
      ConstraintDescription: "Template bucket key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slash (/)."
      Type: "String"
    VPCID:
      Description: "ID for the VPC"
      Type: "AWS::EC2::VPC::Id"
    PublicSubnet1ID:
      Description: "ID of Public Subnet 1"
      Type: "AWS::EC2::Subnet::Id"
    PublicSubnet2ID:
      Description: "ID of Public Subnet 2"
      Type: "AWS::EC2::Subnet::Id"
    PrivateSubnet1ID:
      Description: "ID of Private Subnet 1"
      Type: "AWS::EC2::Subnet::Id"
    PrivateSubnet2ID:
      Description: "ID of Private Subnet 2"
      Type: "AWS::EC2::Subnet::Id"
    EC2LogGroup:
      Description: The bastion log group name
      Type: "String"
    KeyPairName:
      Description: "The name of an existing public/private key pair, which allows you to securely connect to your instance after it launches"
      Type: "AWS::EC2::KeyPair::KeyName"
    RemoteAccessCIDR:
      AllowedPattern: "^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/([0-9]|[1-2][0-9]|3[0-2]))$"
      ConstraintDescription: "CIDR block parameter must be in the form x.x.x.x/x"
      Description: "The CIDR IP range that is permitted to access the AWS resources. It is recommended that you set this value to a trusted IP range."
      Type: "String"
    BastionInstanceType:
      Type: "String"
      Description: "The type of EC2 instance to be launched for Bastion Host"
      AllowedValues:
        # Add more instance types if needed
        - t2.micro
        - t2.medium
        - t2.large
      ConstraintDescription: "Must contain a valid instance type"
    DesiredNumberOfBastionNodes:
      Type: "String"
      MinLength: 1
      Description: "The desired number of Bastion instance to run"
    MaxNumberOfBastionNodes:
      Type: "String"
      MinLength: 1
      Description: "The maximum number of Bastion instances to run"
    MinNumberOfBastionNodes:
      Type: "String"
      MinLength: 1
      Description: "The minimum number of Bastion instances to run"
      Default: "1"
    EKSClusterName:
      Type: String
      Description: The name of the eks cluster
    BastionSecurityGroup:
      Type: String
    BastionInstanceRole:
      Type: String
    BastionInstanceProfile:
      Type: String
    NodeInstanceRoleArn:
      Type: String
    K8sNamespace:
      AllowedPattern: ".+"
      ConstraintDescription: The K8s namespace can not be empty
      Type: String
      Description: The namespace in EKS to deploy kots and datahub app
      Default: "datahub"
    DomainName:
      Type: String
    MySQLEndpoint:
      Type: String
    ElasticSearchEndpoint:
      Type: String
    MSKClusterName: 
      Type: String
    ElbCertArn:
      Type: String
    MasterUserPassword:
      Type: String
    ESMasterUserPassword:
      Type: String

Mappings:
  # see https://github.com/aws-quickstart/quickstart-linux-bastion/blob/master/templates/linux-bastion.template for latest AMI IDs
  # Use Amazon 2 Linux
  BastionLatestAmiRegionMap:
    us-west-2:
      AmiId: ami-0873b46c45c11058d
    us-west-1:
      AmiId: ami-05655c267c89566dd
    us-east-1:
      AmiId: ami-02354e95b39ca8dec
    us-east-2:
      AmiId: ami-07c8bc5c1ce9598c3
    eu-central-1:
      AmiId: ami-0c115dbd34c69a004
    eu-west-1:
      AmiId: ami-07d9160fa81ccffb5
  # Use Amazon 2018
  BastionAmiRegionMap:
    us-east-1:
      AmiId: ami-0080e4c5bc078760e
    us-west-2:
      AmiId: ami-01e24be29428c15b2
    us-east-2:
      AmiId: ami-0cd3dfa4e37921605
    eu-central-1:
      AmiId: ami-0cfbf4f6db41068ac
    eu-west-1:
      AmiId: ami-08935252a36e25f85

Resources:

  # Bastion resources
  #AdminBastionEIP:
  #  Type: AWS::EC2::EIP
  #  Properties:
  #    Domain: vpc

  AdminBastionAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref AdminBastionLaunchConfiguration
      VPCZoneIdentifier:
        - !Ref PublicSubnet1ID
        - !Ref PublicSubnet2ID
      MinSize: !Ref MinNumberOfBastionNodes
      MaxSize: !Ref MaxNumberOfBastionNodes
      Cooldown: "300"
      DesiredCapacity: !Ref DesiredNumberOfBastionNodes
      Tags:
        - Key: Name
          Value: !Sub "${EKSClusterName}-admin-bastion-node"
          PropagateAtLaunch: true
        - Key: Component
          Value: datahub-Bastion-AutoScaling-Group
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT30M

  AdminBastionLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Authentication:
        S3AccessCreds:
          type: S3
          roleName: !Ref BastionInstanceRole
          buckets: !Ref TemplateBucketName
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              awslogs: []
          files:
            '/etc/awslogs/awscli.conf':
              content: !Sub |
                [default]
                region = ${AWS::Region}
                [plugins]
                cwlogs = cwlogs
              mode: '000644'
              owner: root
              group: root
            '/etc/awslogs/awslogs.conf':
              content: !Sub |
                [general]
                state_file = /var/lib/awslogs/agent-state
                [/var/log/bastion/bastion.log]
                file = /var/log/bastion/bastion.log
                datetime_format = %b %d %H:%M:%S
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/bastion/bastion.log
                log_group_name = ${EC2LogGroup}
                [/var/log/dmesg]
                file = /var/log/dmesg
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/dmesg
                log_group_name = ${EC2LogGroup}
                [/var/log/messages]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/messages
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/messages
                log_group_name = ${EC2LogGroup}
                [/var/log/secure]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/secure
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/secure
                log_group_name = ${EC2LogGroup}
                [/var/log/audit/audit.log]
                datetime_format =
                file = /var/log/audit/audit.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/audit/audit.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cron]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/cron
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cron
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-init.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-init.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cfn-init.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-hup.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-hup.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cfn-hup.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-init-cmd.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-init-cmd.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cfn-init-cmd.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cloud-init-output.log]
                file = /var/log/cloud-init-output.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cloud-init-output.log
                log_group_name = ${EC2LogGroup}
                [/var/log/amazon/ssm/amazon-ssm-agent.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/amazon/ssm/amazon-ssm-agent.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/amazon/ssm/amazon-ssm-agent.log
                log_group_name = ${EC2LogGroup}
                [/var/log/amazon/ssm/errors.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/amazon/ssm/errors.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/amazon/ssm/errors.log
                log_group_name = ${EC2LogGroup}
                [/var/log/maillog]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/maillog
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/maillog
                log_group_name = ${EC2LogGroup}
                [/var/log/yum.log]
                datetime_format = %b %d %H:%M:%S
                file = /var/log/yum.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/yum.log
                log_group_name = ${EC2LogGroup}
                [/var/log/awslogs.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/awslogs.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/awslogs.log
                log_group_name = ${EC2LogGroup}
                [/var/log/boot.log]
                file = /var/log/boot.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/boot.log
                log_group_name = ${EC2LogGroup}
                [/var/log/cfn-wire.log]
                datetime_format = %Y-%m-%d %H:%M:%S
                file = /var/log/cfn-wire.log
                log_stream_name = ${AWS::StackName}/bastion-{instance_id}/var/log/cfn-wire.log
                log_group_name = ${EC2LogGroup}
              mode: '000644'
              owner: root
              group: root
            /tmp/bastion_bootstrap.sh:
              source: !Sub "https://s3.${AWS::Region}.amazonaws.com/${TemplateBucketName}/${TemplateBucketKeyPrefix}/scripts/bastion_bootstrap.sh"
              mode: "000550"
              owner: root
              group: root
              authentication: S3AccessCreds
            /tmp/license.yaml:
              source: !Sub "https://s3.${AWS::Region}.amazonaws.com/${TemplateBucketName}/${TemplateBucketKeyPrefix}/license/license.yaml"
              mode: "000550"
              owner: root
              group: root
            /tmp/eks_bootstrap.sh:
              content: !Sub |
                #!/bin/bash
                set -x
                echo "Checking whether cluster exists..."
                aws eks describe-cluster --region ${AWS::Region} --name ${EKSClusterName} &> /dev/null
                if [ $? -eq 0 ]; then
                  echo Updating kubeconfig file...
                  ENDPOINT=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.endpoint --output text)
                  CERT_DATA=$(aws eks describe-cluster --region ${AWS::Region}  --name ${EKSClusterName} --query cluster.certificateAuthority.data --output text)
                  sed -i s,ENDPOINT,$ENDPOINT,g /home/ec2-user/.kube/config
                  sed -i s,CERTIFICATE_DATA,$CERT_DATA,g /home/ec2-user/.kube/config
                  export KUBECONFIG=/home/ec2-user/.kube/config
                  chmod 600 /home/ec2-user/.kube/config
                fi
                echo Install Ingress Controller...
                curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
                mv /tmp/eksctl /usr/local/bin
                eksctl utils associate-iam-oidc-provider --cluster ${EKSClusterName} --approve
                helm repo add eks https://aws.github.io/eks-charts
                helm repo update
                echo "        serviceAccount:" > values.yaml
                echo "          annotations:" >> values.yaml
                echo "            eks.amazonaws.com/role-arn: arn:aws:iam::${AWS::AccountId}:role/datahub-aws-load-balancer-controller" >> values.yaml
                helm install aws-load-balancer-controller eks/aws-load-balancer-controller --set region=${AWS::Region} -n kube-system --set clusterName=${EKSClusterName} --set serviceAccount.name=datahub-aws-load-balancer-controller --set image.tag=v2.2.0 --version 1.2.7 -f values.yaml
                echo prepare configvalues.yaml...
                ZOOKEEPER_CONNECT=$(aws kafka --region ${AWS::Region} list-clusters --cluster-name-filter ${MSKClusterName} --no-paginate --query 'ClusterInfoList[0].ZookeeperConnectString' --output text)
                CLUSTER_ARN=$(aws kafka --region ${AWS::Region} list-clusters --cluster-name-filter ${MSKClusterName} --no-paginate --query 'ClusterInfoList[0].ClusterArn' --output text)
                BOOTSTRAP_BROKERS=$(aws kafka --region ${AWS::Region} get-bootstrap-brokers --cluster-arn $CLUSTER_ARN --no-paginate --query 'BootstrapBrokerString' --output text)
                sed -i "s|BOOTSTRAP_BROKERS|$BOOTSTRAP_BROKERS|g" /home/ec2-user/configvalues.yaml
                sed -i "s|ZOOKEEPER_CONNECT|$ZOOKEEPER_CONNECT|g" /home/ec2-user/configvalues.yaml
                parameter='/${EKSClusterName}/msk/bootstrap_brokers'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "$BOOTSTRAP_BROKERS" --overwrite --no-paginate
                parameter='/${EKSClusterName}/msk/zookeeper_connect'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "$ZOOKEEPER_CONNECT" --overwrite --no-paginate
                parameter='/${EKSClusterName}/mysql/endpoint'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "${MySQLEndpoint}" --overwrite --no-paginate
                parameter='/${EKSClusterName}/elasticsearch/endpoint'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "${ElasticSearchEndpoint}" --overwrite --no-paginate
                echo Creating namespace...
                kubectl create ns ${K8sNamespace}
                echo Generate UUID...
                aws kms generate-random --region ${AWS::Region} --number-of-bytes 32  --output text --query Plaintext>uuid_secret
                aws kms generate-random --region ${AWS::Region} --number-of-bytes 16  --output text --query Plaintext>datahub-password
                DATAHUB_PASSWORD=$(cat datahub-password)
                parameter='/${EKSClusterName}/admin/password'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "SecureString" --value "$DATAHUB_PASSWORD" --overwrite --no-paginate
                echo prepare secrets...
                kubectl get secret/play-secret -n ${K8sNamespace} &> /dev/null
                if [ $? -gt 0 ]; then
                  echo "create secret uuid_secret in namespace: ${K8sNamespace}"
                  kubectl create secret generic play-secret --from-file=uuid_secret=uuid_secret --namespace ${K8sNamespace}
                fi
                kubectl create secret generic datahub-secrets --from-file=datahub-password=datahub-password --namespace ${K8sNamespace}
                echo -n "${MasterUserPassword}" > mysql-password
                echo -n "${ESMasterUserPassword}" > elasticsearch-password
       
                kubectl create secret generic mysql-secrets --from-file=mysql-password=mysql-password --namespace ${K8sNamespace}
                kubectl create secret generic elasticsearch-secrets --from-file=elasticsearch-password=elasticsearch-password --namespace ${K8sNamespace}
                echo Cleanup eks oidc...
                parameter='/${EKSClusterName}/eks/oidc'
                aws ssm delete-parameter --region ${AWS::Region} --name "$parameter" --no-paginate
                echo Install kots...
                kubectl kots install datahub-poc/unstable -n ${K8sNamespace} --shared-password Passw0rd --license-file /tmp/license.yaml --config-values /home/ec2-user/configvalues.yaml --wait-duration 120s --port-forward=false
                echo Install kotsadmin svc...
                kubectl apply -f /home/ec2-user/kotsadm-svc.yaml
                echo Waiting datahub-kotsadm to be Active...
                aws elbv2 describe-load-balancers --region ${AWS::Region} --names datahub-kotsadm
                while [ $? -gt 0 ]; do
                  echo datahub-kotsadm is still creating, sleeping for 30 seconds...
                  sleep 30
                  aws elbv2 describe-load-balancers --region ${AWS::Region} --names datahub-kotsadm
                done
                STATUS=$(aws elbv2 describe-load-balancers --region ${AWS::Region} --names datahub-kotsadm --query LoadBalancers[0].State.Code --output text --no-paginate)
                while [ $STATUS != 'active' ]; do
                  echo datahub-kotsadm is still provisioning, sleeping for 10 seconds...
                  sleep 10
                  STATUS=$(aws elbv2 describe-load-balancers --region ${AWS::Region} --names datahub-kotsadm --query LoadBalancers[0].State.Code --output text --no-paginate)
                done
                NLBARN=$(aws elbv2 describe-load-balancers --region ${AWS::Region} --names datahub-kotsadm --query LoadBalancers[0].LoadBalancerArn --output text --no-paginate)
                parameter='/${EKSClusterName}/admin/nlbarn'
                aws ssm put-parameter --region ${AWS::Region} --name "$parameter" --type "String" --value "$NLBARN" --overwrite --no-paginate
              mode: "000750"
              owner: root
              group: root
            /home/ec2-user/.kube/config:
              content: !Sub |
                apiVersion: v1
                clusters:
                - cluster:
                    server: ENDPOINT
                    certificate-authority-data: CERTIFICATE_DATA
                  name: kubernetes
                contexts:
                - context:
                    cluster: kubernetes
                    user: aws
                  name: aws
                current-context: aws
                kind: Config
                preferences: {}
                users:
                - name: aws
                  user:
                    exec:
                      apiVersion: client.authentication.k8s.io/v1alpha1
                      command: aws-iam-authenticator
                      args:
                        - token
                        - -i
                        - ${EKSClusterName}
              mode: "000666"
              owner: ec2-user
              group: ec2-user
            /home/ec2-user/kotsadm-svc.yaml:
              content: !Sub |
                apiVersion: v1
                kind: Service
                metadata:
                  annotations:
                    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
                    service.beta.kubernetes.io/aws-load-balancer-name: datahub-kotsadm
                    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
                    service.beta.kubernetes.io/aws-load-balancer-scheme: internal
                    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "${ElbCertArn}"
                    service.beta.kubernetes.io/aws-load-balancer-type: external
                  labels:
                    kots.io/kotsadm: "true"
                  name: kotsadm-nlb
                  namespace: datahub
                spec:
                  ports:
                  - name: https
                    port: 443
                    targetPort: 3000
                  selector:
                    app: kotsadm
                  sessionAffinity: None
                  type: LoadBalancer
              mode: "000666"
              owner: ec2-user
              group: ec2-user
            /home/ec2-user/configvalues.yaml:
              content: !Sub |
                apiVersion: kots.io/v1beta1
                kind: ConfigValues
                metadata:
                  name: datahub_app
                spec:
                  values:
                    domain:
                      repeatableItem: service_port
                      value: "${DomainName}"
                    lr_order:
                      repeatableItem: service_port
                      value: "200"
                    mysql_endpoint:
                      repeatableItem: service_port
                      value: "${MySQLEndpoint}"
                    elasticsearch_endpoint:
                      repeatableItem: service_port
                      value: "${ElasticSearchEndpoint}"
                    bootstrap_brokers:
                      repeatableItem: service_port
                      value: BOOTSTRAP_BROKERS
                    zookeeper_connect:
                      repeatableItem: service_port
                      value: ZOOKEEPER_CONNECT
                    certificate_arn:
                      repeatableItem: service_port
                      value: "${ElbCertArn}"
              mode: "000666"
              owner: ec2-user
              group: ec2-user
          services:
            sysvinit:
              awslogs:
                enabled: true
                ensureRunning: true
                packages:
                  yum:
                  - awslogs
                files:
                - '/etc/awslogs/awslogs.conf'
                - '/etc/awslogs/awscli.conf'
          commands:
            01_eks-bootstrap:
              command: "./tmp/eks_bootstrap.sh |tee -a /tmp/eks_bootstrap.output 2>&1"
            02_bastion-bootstrap:
              command: "./tmp/bastion_bootstrap.sh --tcp-forwarding false --x11-forwarding false|tee -a /tmp/bastion_bootstrap.output 2>&1"
    Properties:
      AssociatePublicIpAddress: true
      PlacementTenancy: default
      KeyName: !Ref KeyPairName
      IamInstanceProfile: !Ref BastionInstanceProfile
      ImageId: !FindInMap [BastionAmiRegionMap, !Ref "AWS::Region", AmiId]
      SecurityGroups:
        - !Ref BastionSecurityGroup
      InstanceType: !Ref BastionInstanceType
      UserData:
        Fn::Base64: !Sub |
            #!/bin/bash
            export PATH=$PATH:/usr/local/bin
            yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
            yum install -y git
            pip install awscli --upgrade
            easy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz
            CLOUDWATCHGROUP=${EC2LogGroup}
            curl -LO https://dl.k8s.io/release/v1.20.2/bin/linux/amd64/kubectl
            chmod +x ./kubectl
            mv kubectl /usr/local/bin
            kubectl version --short --client
            curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/aws-iam-authenticator
            chmod +x ./aws-iam-authenticator
            mv aws-iam-authenticator /usr/local/bin
            aws-iam-authenticator version
            curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
            chmod +x ./get_helm.sh
            ./get_helm.sh
            helm version --client
            wget https://github.com/replicatedhq/kots/releases/download/v1.55.0/kots_linux_amd64.tar.gz
            tar xzvf kots_linux_amd64.tar.gz
            mv -f kots /usr/local/bin/kubectl-kots
            kubectl kots --help
            cfn-init -v --stack ${AWS::StackName} --resource AdminBastionLaunchConfiguration --region ${AWS::Region}
            cfn-signal -e $? --stack ${AWS::StackName} --resource AdminBastionAutoScalingGroup --region ${AWS::Region}

Outputs:
  SubstackName:
    Description: The admin stack name
    Value: !Sub "${AWS::StackName}"
  AdminBastionName:
    Description: The Admin Bastion ec2 Name tag
    Value: "datahub-admin-bastion-node"
  AdminBastionKeyPair:
    Description: SSH Key Pair to access Admin Bastion
    Value: !Ref KeyPairName
  AdminBastionUser:
    Description: SSH username to access Admin Bastion
    Value: "ec2-user"
  DatahubAppUrl:
    Description: Datahub Application Url
    Value: !Sub "https://${DomainName}"
  DatahubUser:
    Description: Datahub Username
    Value: "admin"
  DatahubPassword:
    Description: Datahub password
    Value: "Check SSM Parameter Store: /datahub/admin/password"
  AuroraRDSEndpoint:
    Description: Aurora RDS endpoint
    Value: "Check SSM Parameter Store: /datahub/mysql/endpoint"
  AuroraRDSUser:
    Description: Aurora RDS master username
    Value: "Check SSM Parameter Store: /datahub/mysql/username"
  AuroraRDSPasswordj:
    Description: Aurora RDS master password
    Value: "Check SSM Parameter Store: /datahub/mysql/password"
  ElasticSearchEndpoint:
    Description: ElasticSearch endpoint
    Value: "Check SSM Parameter Store: /datahub/elasticsearch/endpoint"
  ElasticSearchUser:
    Description: ElasticSearch master username
    Value: "Check SSM Parameter Store: /datahub/elasticsearch/username"
  ElasticSearchPassword:
    Description: ElasticSearch master password
    Value: "Check SSM Parameter Store: /datahub/elasticsearch/password"
